{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MRC0e0KhQ0S"
   },
   "source": [
    "# Support Vector Machine (SVM)\n",
    "- Created in the 1960s. They are popular because are powerful and different from other ML learning algorithms.\n",
    "- Imagina you have a plot and there are two main clusters, how can you separate the data? Maybe drawing a line between the two clusters to classify them. And there are many different lines that can separate the results, but will have different results when adding new data. So, need to find best line to separate the data.\n",
    "- Line is found through the maximum margin, so draw a line and will choose the one that separates better the clusters by optimizing the separation between the line and the distance with the nearest point of the two clusters.\n",
    "- The SVM checks the nearest points from the different clusters and calculates the margin based on the nearest vectors (in 3D/2D this are points, but higher dimensions then a point is represented like a vector). That is why its called support vector machine.\n",
    "- In 2D/3D it can be seen as a line that separates the points. But higher dimensions is a hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, when data points are in a cluster, you can see that usually when points of different clusters are very close means that they are similar or hard to recognize. In contrast, the points that are in the center of each cluster or far away from the other clusters are the data points that are easy for the model to recognize as part of that cluster. So, what SVM does is look for those points for each cluster that are hard to differentiate because are close to the other clusters and try to maximize the distance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWd1UlMnhT2s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1735122767840,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -60
    },
    "id": "YvGPUQaHhXfL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1VMqkGvhc3-"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 400 rows if a customer bought 'yes' or 'no' SUV. Using age and salary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735122767842,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -60
    },
    "id": "M52QDmyzhh9s"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  EstimatedSalary  Purchased\n",
       "0   19            19000          0\n",
       "1   35            20000          0\n",
       "2   26            43000          0\n",
       "3   27            57000          0\n",
       "4   19            76000          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvxIPVyMhmKp"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2339,
     "status": "ok",
     "timestamp": 1735122770177,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -60
    },
    "id": "AVzJWAXIhxoC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW3c7UYih0hT"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1735122770180,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -60
    },
    "id": "9fQlDPKCh8sc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train,)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb6jCOCQiAmP"
   },
   "source": [
    "## Training the SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel='linear', random_state=0) # classic model with the linear kernel\n",
    "model = classifier.fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKYVQH-l5NpE"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4Hwj34ziWQW"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix\n",
    "- 0,0: True negatives - negatives that were actually negatives\n",
    "- 1,0: False negatives - values that were predicted false but where positive\n",
    "- 0,1: False positives - values that were predicted positive but where false\n",
    "- 1,1: True positives - positives that were actually positives\n",
    "\n",
    "Accuracy \n",
    "- Measure in machine learning of how many predictions that you did were actually true\n",
    "\n",
    "F1 Score\n",
    "- Combines the precision and recall of a model. This is better when classes are imbalanced because when classes are imbalanced and calculate accuracy, then it doesn't represent the predicitive power of the model. i.e. If have that 80% of predictions are true, and only 20% are false, then here you have a class imbalance and the model could learn to always predict true all the time, and accuracy (# of correct predictions) will give .80 which is big considering that you are only printing true.\n",
    "\n",
    "- On classification problems, precision and recall are metrics that come at the cost of another.\n",
    "- Precision: (sum of true positives) / (sum of true positives + sum of false positives) -> Of all the positives you predicted, how many were correct. This doubts even positives, because don't want to be incorrect. Optimize to not be incorrect, so decrease recall. \n",
    "- Recall: (sum of true positives) / (sum of true positives + sum of false negatives) -> What percentage of all the true values you predicted to true. This one is less critic because want to catch all the trues. Optiimzing for selecting the actual correct ones, so decrease precision\n",
    "\n",
    "- F1 Score: Uses harmonic mean to maximize precision and recall. Which is (2 * precision * recall) / (precision + recall). Harmonic mean encourages similar values for both precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Print the confusion matrix, accuracy, and the f1 score.\n",
    "- The results can be improved if the kernel method used with SVM is non linear. This is because in here we are using linear models and that maybe not be the best method for fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57,  1],\n",
       "       [ 6, 16]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205128205128205"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OMC_P0diaoD"
   },
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is find the decision boundary to clearly separate the data. And most of the time this can be not done through a linear function. Based on the kernel that you are doing, is the assumption if the data is linearly separable, polynomic, etc. \n",
    "\n",
    "The idea is to create a mapping function that takes the data that is not linearly separable, and increasing the dimensions that you have, making the data higher dimension. In exmaple, if have data plotted in 2D then use a mapping function to transform it to 3D. By this, it is possible that the result makes the data separable by a linear function, even when before it was not possible. The problem with mapping to higher dimensions is that is computationa expensive. For this, we use the kernel trick that makes similar results but without being that much of computational expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "- Gaussian RBF Kernel: Python locates a landmark on the clusters of points and is able to take all the points within a circumference and be able to differentiate between the circumference and other values. The value of sigma in this case is the value of the diameter of the circle. And this allows to separate the data in the same dimension, not in higher dimensions. And in the same way, if have more complex numbers, like the infinte symbol inside a circle (imagining that inside infinte symbol there is one result), then we can convert two of this methods to fit the data.\n",
    "- Sigmoid kernel: still select a landmark and depending on the distance, will give values between 0-1.\n",
    "- Polynomial kernel: The kernel behaves more like a polynomial function with 3 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOOi7yVtSStc0z4wlOS+D9T",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
